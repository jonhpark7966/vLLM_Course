


# Contents


- vLLM vs Other Platforms
- Nvidia Triton
- vLLM OpenAI compatible serving
- serverless
- Distributed Inference (Multi GPU & Multi Node)
- Scaling up
- Quantization
- Own Model