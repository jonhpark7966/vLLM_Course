{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Contents","text":"<ul> <li>vLLM vs Other Platforms</li> <li>Nvidia Triton</li> <li>vLLM OpenAI compatible serving</li> <li>Multi GPU</li> <li>servereless</li> <li>Scaling up</li> <li>Quantization</li> <li>Own Model</li> </ul>"},{"location":"LoRA/","title":"LoRA Adapters","text":"<p>vLLM base model\uc5d0\u00a0LoRA adapters\u00a0\ub97c \uc62c\ub9ac\uaca0\uc2b5\ub2c8\ub2e4. vLLM model \ub4e4\uc774\u00a0<code>SupportsLoRA</code> \ub8f0 \uad6c\ud604\ud574 \ub480\uc73c\uba74, LoRA\ub97c \ubd99\uc5ec \uc4f8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  </p> <p>\ub85c\ub77c\ub294 request \ub9c8\ub2e4 \uc801\uc6a9 \uc2dc\ucf1c\uc11c \uc11c\ube59\ud560 \ub54c \uc88b\uc544\uc694.</p> <p>SQL \uc6a9 LoRA adapter \ub97c \ub2e4\uc6b4 \ubc1b\uc544\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <pre><code>from huggingface_hub import snapshot_download\n\nsql_lora_path = snapshot_download(repo_id=\"yard1/llama-2-7b-sql-lora-test\")\n</code></pre> <pre><code>huggingface-cli download yard1/llama-2-7b-sql-lora-test\n</code></pre> <p>\ubd80\ubaa8 \ubaa8\ub378\uc778 Llama-2-7b-hf \ubaa8\ub378\uacfc \ud568\uaed8 \uc11c\ube59\uc744 \ud574\ubd05\ub2c8\ub2e4.</p> <p>commit ID\ub294 \ubc14\ub014 \uc218 \uc788\uc73c\ub2c8 \uaf2d \ud655\uc778\ud558\uc138\uc694!</p> <pre><code>vllm serve meta-llama/Llama-2-7b-hf \\\n    --enable-lora \\\n    --lora-modules sql-lora=$HOME/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c/\n</code></pre> <p>completion endpoint\ub85c \uc694\uccad\uc744 \ub0a0\ub9b4 \ub54c, model\uc5d0 lora adapter \uc774\ub984\uc744 \ub123\uc5b4\uc8fc\uba74 \ub429\ub2c8\ub2e4.</p> <pre><code>curl http://localhost:8000/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"sql-lora\",\n        \"prompt\": \"San Francisco is a\",\n        \"max_tokens\": 7,\n        \"temperature\": 0\n    }'\n</code></pre> <p>24\ub144 10\uc6d4 \uae30\uc900, \ud544\uc790\uc758 \uac1c\uc778\uc801\uc778 \uc758\uacac.  \uc88b\uc740 lora \ubaa8\ub378\uc744 \ucc3e\uae30\ub3c4 \ud798\ub4e4\uace0, \ud6a8\uacfc\ub97c \ubcf8 \uacbd\ud5d8\uc774 \uc5c6\uc2b5\ub2c8\ub2e4. diffusion \uacc4\uc5f4\uc5d0\uc11c\ub294 \ub9ce\uc774 \uc88b\uc740 \ubaa8\uc2b5\uc744 \ubcf4\uc558\ub294\ub370, llm \uc5d0\uc11c\ub294 \uc27d\uc9c0 \uc54a\ub124\uc694. \ud2b9\uc815 \ubaa9\uc801\uc73c\ub85c \uc9c1\uc811 lora\ub97c \ub9cc\ub4e4\uc5b4\ubd10\uc57c \ub354 \ud310\ub2e8\uc774 \uac00\ub2a5\ud560 \ub4ef \ud569\ub2c8\ub2e4.</p>"},{"location":"Metrics_Prometheus%26Grafana/","title":"vLLM \ud504\ub85c\uba54\ud14c\uc6b0\uc2a4 &amp; \uadf8\ub77c\ud30c\ub098 \ubaa8\ub2c8\ud130\ub9c1","text":"<p>vllm \uc744 \uc11c\ube59\ud558\uba74 <code>/metrics</code> endpoint \uc5d0\uc11c \uc9c0\ud45c\ub4e4\uc744 \uad00\ucc30 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <pre><code># HELP vllm:num_requests_running Number of requests currently running on GPU.\n# TYPE vllm:num_requests_running gauge\nvllm:num_requests_running{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\n# HELP vllm:num_requests_swapped Number of requests swapped to CPU.\n# TYPE vllm:num_requests_swapped gauge\nvllm:num_requests_swapped{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\n# HELP vllm:num_requests_waiting Number of requests waiting to be processed.\n# TYPE vllm:num_requests_waiting gauge\nvllm:num_requests_waiting{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\n# HELP vllm:gpu_cache_usage_perc GPU KV-cache usage. 1 means 100 percent usage.\n# TYPE vllm:gpu_cache_usage_perc gauge\nvllm:gpu_cache_usage_perc{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\n# HELP vllm:cpu_cache_usage_perc CPU KV-cache usage. 1 means 100 percent usage.\n# TYPE vllm:cpu_cache_usage_perc gauge\nvllm:cpu_cache_usage_perc{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\n# HELP vllm:cpu_prefix_cache_hit_rate CPU prefix cache block hit rate.\n# TYPE vllm:cpu_prefix_cache_hit_rate gauge\nvllm:cpu_prefix_cache_hit_rate{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} -1.0\n# HELP vllm:gpu_prefix_cache_hit_rate GPU prefix cache block hit rate.\n# TYPE vllm:gpu_prefix_cache_hit_rate gauge\nvllm:gpu_prefix_cache_hit_rate{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} -1.0\n# HELP vllm:avg_prompt_throughput_toks_per_s Average prefill throughput in tokens/s.\n# TYPE vllm:avg_prompt_throughput_toks_per_s gauge\nvllm:avg_prompt_throughput_toks_per_s{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\n# HELP vllm:avg_generation_throughput_toks_per_s Average generation throughput in tokens/s.\n# TYPE vllm:avg_generation_throughput_toks_per_s gauge\nvllm:avg_generation_throughput_toks_per_s{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\n# HELP vllm:cache_config_info Information of the LLMEngine CacheConfig\n# TYPE vllm:cache_config_info gauge\nvllm:cache_config_info{block_size=\"16\",cache_dtype=\"auto\",cpu_offload_gb=\"0\",enable_prefix_caching=\"False\",gpu_memory_utilization=\"0.9\",num_cpu_blocks=\"2048\",num_gpu_blocks=\"2375\",num_gpu_blocks_override=\"None\",sliding_window=\"None\",swap_space_bytes=\"4294967296\"} 1.0\n# HELP vllm:num_preemptions_total Cumulative number of preemption from the engine.\n# TYPE vllm:num_preemptions_total counter\nvllm:num_preemptions_total{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\n# HELP vllm:prompt_tokens_total Number of prefill tokens processed.\n# TYPE vllm:prompt_tokens_total counter\nvllm:prompt_tokens_total{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\n# HELP vllm:generation_tokens_total Number of generation tokens processed.\n# TYPE vllm:generation_tokens_total counter\nvllm:generation_tokens_total{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\n</code></pre> <p>\ucc38\uace0: vllm backend \ub97c \uc0ac\uc6a9\ud558\ub294 nvidia triton \ucee8\ud14c\uc774\ub108\ub294 24.08 \ubc84\uc804 \ubd80\ud130 \uc9c0\uc6d0\ud558\ub2c8 \ucc38\uace0\ud558\uc138\uc694.</p>"},{"location":"Metrics_Prometheus%26Grafana/#server-setup","title":"Server Setup","text":"<p>\uc704 \uc9c0\ud45c\ub294 Promethous \ud615\uc2dd\uc774\ub77c, k8s \uc5d0\uc11c \ub9ce\uc774\ub4e4 \uc0ac\uc6a9\ud558\ub294 grafana\ub85c \uc62e\uaca8 \ub300\uc2dc\ubcf4\ub4dc\ub97c \ubaa8\ub294 \uac83\uc774 \ub9e4\uc6b0 \uc27d\uc2b5\ub2c8\ub2e4.</p> <p>vllm \uacf5\uc2dd \ub808\ud3ec\uc5d0\uc11c\ub294 promethous &amp; grafana \uc11c\ube59\uc744 \uc704\ud55c docker-compose.yaml \ud30c\uc77c\uc744 \uc81c\uacf5\ud558\uae30 \ub584\ubb38\uc5d0 \uc27d\uac8c \uc14b\uc5c5\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <p>\ubaa8\ub378\uc744 \uc11c\ube59 \ud55c \ud6c4 <pre><code>vllm serve {MODEL} \\\n    --max-model-len 2048 \\\n    --disable-log-requests\n</code></pre></p> <p>\ub3c4\ucee4\ub85c \uc11c\ubc84\ub97c \uc62c\ub9bd\ub2c8\ub2e4.</p> <pre><code>docker compose up\n</code></pre> <p>\uc774\uc81c \ud14c\uc2a4\ud2b8\ub97c \ub0a0\ub824\uc8fc\uba74 \ub418\ub294\ub370, sharegpt \ub85c \ub0a0\ub9ac\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4, \uc2e4\uc81c \uc6b4\uc601 \uc0c1\ud669\uc5d0\uc11c\ub294 \uc720\uc800\ub4e4\uc774 \uc694\uccad\uc744 \ub0a0\ub9ac\uace0 \uc788\uaca0\uc8e0.</p> <pre><code>wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\npython3 ../../benchmarks/benchmark_serving.py \\\n    --model mistralai/Mistral-7B-v0.1 \\\n    --tokenizer mistralai/Mistral-7B-v0.1 \\\n    --endpoint /v1/completions \\\n    --dataset-name sharegpt \\\n    --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\n    --request-rate 3.0\n</code></pre> <p>\uc704\uc640 \uac19\uc774 \u00a0<code>http://localhost:8000/metrics</code> \ub9c1\ud06c\uc5d0\uc11c prometheous \ub370\uc774\ud130\ub97c \ubcfc \uc218 \uc788\uace0\uc694.</p>"},{"location":"Metrics_Prometheus%26Grafana/#grafana","title":"Grafana","text":"<p>Grafana \uc14b\uc5c5\uc744 \ud574\uc11c \ub370\uc774\ud130\ub97c \uc5f0\ub3d9, visualize \ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <p><code>http://localhost:3000</code>  \ub85c \ub4e4\uc5b4\uac00 \uae30\ubcf8 \uc14b\ud305\uc73c\ub85c \ub85c\uadf8\uc778 \ud569\ub2c8\ub2e4.  username (<code>admin</code>), password (<code>admin</code>).</p>"},{"location":"Metrics_Prometheus%26Grafana/#_1","title":"\ud504\ub85c\uba54\ud14c\uc6b0\uc2a4 \ub370\uc774\ud130 \uc18c\uc2a4 \uc5f0\uacb0","text":"<p><code>http://localhost:3000/connections/datasources/new</code>\u00a0\ub85c \ub458\uc5b4\uac00 \ud504\ub85c\uba54\ud14c\uc6b0\uc2a4\ub97c \uc120\ud0dd\ud574 \ub370\uc774\ud130\ub97c \uc774\uc5b4\uc90d\ub2c8\ub2e4.  </p> <p><code>Prometheus Server URL</code>\u00a0\uc744 \uc5f0\uacb0\ud574\uc918\uc57c \ud558\ub294\ub370, \ud604\uc7ac\ub294 \ub2e4\ub978 \ucee8\ud14c\uc774\ub108\uac00 \uac01\uc790 \ub450\uac1c \ub5a0 \uc788\uc9c0\ub9cc, \ub3c4\ucee4\uac00 DNS\ub97c \uc5f0\uacb0\ud574 \uc8fc\ub3c4\ub85d \uc14b\uc5c5\uc774 \ub418\uc5b4\uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub0e5 <code>http://prometheus:9090</code> \ub9cc \uc5f0\uacb0\ud574\uc8fc\uba74 \ub429\ub2c8\ub2e4.</p> <p><code>Save &amp; Test</code>\ub97c \ub204\ub974\uba74,  \"Successfully queried the Prometheus API.\" \ub77c\uace0 \ub730 \uac81\ub2c8\ub2e4.</p>"},{"location":"Metrics_Prometheus%26Grafana/#_2","title":"\ub300\uc2dc\ubcf4\ub4dc \uc14b\uc5c5","text":"<p><code>http://localhost:3000/dashboard/import</code> \ub85c \uc811\uc18d\ud574\uc11c\u00a0<code>grafana.json</code> \ud30c\uc77c\uc744 \uc5c5\ub85c\ub4dc \ud574\uc90d\ub2c8\ub2e4. <code>prometheus</code>\u00a0\ub370\uc774\ud130 \uc18c\uc2a4\ub97c \uc120\ud0dd\ud574\uc8fc\uace0\uc694. \uadf8\ub7ec\uba74 \uc0ac\uc804 \uc14b\uc5c5\ub41c \ub300\uc2dc\ubcf4\ub4dc\uac00 \ud45c\uc2dc\uac00 \ub429\ub2c8\ub2e4.</p>"},{"location":"Metrics_Prometheus%26Grafana/#runpod","title":"Runpod \uc5d0\uc11c \uc5f0\ub3d9 \uc2dc\ucf1c\ubcf4\uae30","text":"<p>\uc9c1\uc811 docker \ub97c \ub9d0\uae30 \uc804\uc5d0, konuu/llm_ready:latest image \uc5d0\uc11c metric \uc5f0\ub3d9\ud558\ub294 \uc791\uc5c5\uc744 \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc0c1\ud669\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4, \ubb3c\ub860 \uc774\ub807\uac8c \uc548 \ud574\ub3c4 \ub429\ub2c8\ub2e4. \uc11c\ubc84\uc758 \uc704\uce58\ub294 \uc6d0\ud558\ub294 \ub300\ub85c \ubc14\uafc0 \uc218 \uc788\uc8e0. - runpod \uc5d0\uc11c prometheous \ub97c \uc62c\ub9bd\ub2c8\ub2e4. - \ub2e4\ub978 grafana \uc11c\ubc84\uc5d0\uc11c \ubaa8\uc544\uc11c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.</p> <ol> <li> <p>Pod edit \ud574\uc11c 8888,8000,3000,9090 \ud3ec\ud2b8\ub97c \ub6ab\uc5b4\uc90d\ub2c8\ub2e4. (3000\uc740 \uadf8\ub77c\ud30c\ub098\ub77c\uc11c \ub2e4\ub978 \uacf3\uc5d0 \uc62c\ub9b4 \uac83\uc774\uba74 \uc548\ub6ab\uc5b4\uc918\ub3c4 \ub429\ub2c8\ub2e4.)</p> </li> <li> <p>vllm \uc744 \ucf1c\uc90d\ub2c8\ub2e4. <pre><code>vllm serve Qwen/Qwen2.5-7B-Instruct\n</code></pre></p> </li> <li> <p>prometheus \ub97c \uc124\uce58\ud569\ub2c8\ub2e4. <pre><code>wget https://github.com/prometheus/prometheus/releases/download/v2.53.2/prometheus-2.53.2.linux-amd64.tar.gz\ntar -xvzf ./pro...tar.gz\n</code></pre></p> </li> <li> <p>vllm repo \uc5d0\uc11c prometheous.yml \uc744 \ubc1b\uc544\uc11c \ucf1c\uc90d\ub2c8\ub2e4.</p> </li> <li>https://github.com/vllm-project/vllm/blob/main/examples/production_monitoring/prometheus.yaml</li> </ol> <pre><code>cat &lt;&lt;EOF &gt; prometheus.yaml\n# prometheus.yaml\nglobal:\n  scrape_interval: 5s\n  evaluation_interval: 30s\n\nscrape_configs:\n  - job_name: vllm\n    static_configs:\n      - targets:\n          - 'localhost:8000'\nEOF\n</code></pre> <pre><code>./prometheus --config.file=prometheus.yaml\n</code></pre> <ol> <li>\ub2e4\ub978 \uacf3 \uc5b4\ub518\uac00\uc5d0\uc11c grafana\ub97c \ud0a4\uace0 \uc5f0\uacb0\uc744 \ud574\uc90d\uc2dc\ub2e4.</li> </ol> <p>ex) https://l3wuat7zoyx44d-9090.proxy.runpod.net/</p> <p>\uadf8\ub9ac\uace0 \uc77c\uc744 \ud558\uba74 \ubaa8\ub2c8\ud130\ub9c1\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <p></p>"},{"location":"MultiGPU/","title":"MultiGPU","text":"<p>\uc694\uc998\uc758 LLM \uc740 \ud544\uc5f0\uc801\uc73c\ub85c \uba40\ud2f0 GPU \ud639\uc740 \uba40\ud2f0 \ub178\ub4dc \ub85c \ud655\uc7a5\ub41c Inference \uac00 \ud544\uc694\ud558\uc8e0.  GPU\ub294 \ud558\ub098\uc758 LLM \uc744 \ub3cc\ub824\uc8fc\uae30\uc5d0 \uba54\ubaa8\ub9ac\uac00 \ubaa8\uc790\ub974\ub2c8\uae4c\uc694.   </p>"},{"location":"MultiGPU/#multi-gpu","title":"Multi - GPU","text":"<p>\ud55c \ub178\ub4dc\uc5d0 \uc5ec\ub7ec GPU \uac00 \uc5ec\ub7ec\uac1c \uaf3d\ud600\uc788\ub294 \uacbd\uc6b0\uc785\ub2c8\ub2e4.  \uc0ac\uc2e4 \uc774 \uacbd\uc6b0\ub294 \ub9e4\uc6b0 \uc26c\uc6cc\uc694. \uadf8\ub0e5 \uc635\uc158\ub9cc \uc8fc\uba74 \ub429\ub2c8\ub2e4.  </p> <pre><code>from vllm import LLM\nllm = LLM(\"meta-llama/Llama-3.1-8B\", tensor_parallel_size=4)\noutput = llm.generate(\"San Franciso is a\")\n</code></pre> <pre><code>vllm serve meta-llama/Llama-3.1-8B \\\n    --tensor-parallel-size 4\n</code></pre> <p>\ud30c\uc774\uc120 \ub610\ub294 shell \ucee4\ub9e8\ub4dc\ub85c \uc704\uc640 \uac19\uc774 \uc8fc\uba74  \ub05d\uc785\ub2c8\ub2e4. \ucc38 \uc27d\uc8e0?  </p> <p>\uc5ec\uae30\uc11c \ub2e4\ub978 \uc635\uc158\uc774 \ud558\ub098\ub354 \uc788\ub294\ub370\uc694.  <code>pipeline parallelism</code> \uc785\ub2c8\ub2e4. \ubcd1\ub82c \ub2e8\uacc4\ub97c \ud150\uc11c\ub808\ubca8\uc5d0\uc11c \uc8fc\ub294 \uac83\uc774 \uc544\ub2c8\ub77c \ub808\uc774\uc5b4 \ub808\ubca8\uc5d0\uc11c \uc8fc\ub294 \uac83\uc785\ub2c8\ub2e4. Tensor parallelism vs Pipeline Parallelim \uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ub2ec\ub824\uc788\ub294 \ub9c1\ud06c\ub97c \ucc38\uc870\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4.  </p> <p>\uc608\ub97c \ub4e4\uc5b4 \ud55c \ub178\ub4dc\uc5d0 \ud604\uc7ac gpu \uac00 8\uac1c \uc788\ub2e4\uace0 \ud574 \ubd05\uc2dc\ub2e4. \uadf8\ub7ec\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud558\ub294 \ubc29\ubc95\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.  </p> <pre><code>vllm serve meta-llama/Llama-3.1-8B \\\n    --tensor-parallel-size 4 \\\n    --pipeline-parallel-size 2\n</code></pre> <p>\uc774\ub807\uac8c run \uc744 \ud574\uc8fc\uba74 \ud558\ub098\uc758 matrix \uc758 \uc5f0\uc0b0\uc5d0 \ub300\ud574\uc11c\ub294 4\uac1c\uc758 gpu\ub97c \uc4f0\uace0, Layer\uac00 \ub118\uc5b4\uac00\uba74 \ub2e4\uc74c 4\uac1c\uc758 gpu \ub97c \uc0ac\uc6a9\ud558\ub2c8\uae4c, gpu \uac19\uc758 \ud1b5\uc2e0 \ub85c\uc9c1\uc774 \ud544\uc694\ud55c \uacbd\uc6b0\uac00 \uc904\uaca0\uc8e0. \ub300\uc2e0, \uc5f0\uc18d\ub41c task \uac00 \uc5c6\ub2e4\uba74 \ub178\ub294 gpu \ub4e4\uc774 \uc0dd\uae38 \uac83 \uc785\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c\ub294 high throughput \uc744 \ub2ec\uc131 \ud560 \uc218 \uc788\uc744 \ud150\ub370, \ubaa8\ub378 \ub530\ub77c input token \ub530\ub77c \ucc28\uc774\uac00 \uc788\uc744\ud14c\ub2c8 \ud655\uc815\uc801\uc778 \uac83\uc740 \uc544\ub2d9\ub2c8\ub2e4. </p>"},{"location":"MultiGPU/#_1","title":"\uc9c1\uc811 \ub3cc\ub824\ubcf4\uae30!","text":"<p>A40 (48GB) x2 \ub97c \ub300\uc5ec\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ub300\uc5ec\ub294 runpod \uc5d0\uc11c \ud588\uace0\uc694, \uc11c\ube44\uc2a4 \uc774\ub984\uc5d0\uc11c \uc54c\uc218 \uc788\ub4ef  <code>konuu/llm_ready:latest</code> docker image \ub97c \uc62c\ub824 \uc90d\ub2c8\ub2e4.  vllm image \ub97c \uc548 \uc62c\ub9b0 \uc774\uc720\ub294, model\uc744 \ucc98\uc74c\ubd80\ud130 \uac19\uc774 \ub744\uc6cc\uc8fc\ub294\uac8c \uc2eb\uc5b4\uc11c \uc785\ub2c8\ub2e4.</p> <p>(\ucc38\uace0) RTX3090 (24GB) x 4 \ub97c \uba3c\uc800 \ub300\uc5ec\ud588\ub294\ub370 \uc5d0\ub7ec\uac00 \ub098\uc11c \uc548\ub728\ub294 \uad70\uc694... ;;</p> <p>\uac1c\uc778\uc801\uc73c\ub85c\ub294 container \uac00 \uc62c\ub77c\uac04 \uac83\uc774 \uc544\ub2c8\ub77c \ud558\ub4dc\uc6e8\uc5b4\ub97c \ube4c\ub824\uc8fc\ub294 LambdaLabs \ub97c \uc120\ud638\ud558\ub294\ub370, \uc694\uc998 GPU \uac00 \ub3d9\ub098\uc11c... \ube4c\ub9ac\uae30\uac00 \ud798\ub4dc\ub124\uc694.</p> <p></p> <p>\uc11c\ube59\uc744 \ud55c\ubc88 \ud574\ubcf4\uc8e0, \ud658\uacbd\ubcc0\uc218 \uc14b\uc5c5\uc744 \ud55c\ubc88 \ud569\ub2c8\ub2e4.  meta-llama \ub294 \ud5c8\uae45\ud398\uc774\uc2a4\uc5d0\uc11c \uad8c\ud55c\uc744 \ubc1b\uc544\uc57c\ub9cc \uc0ac\uc6a9\uc774 \uac00\ub2a5\ud558\ub2c8, \ubbf8\ub9ac \uc2b9\uc778\uc744 \ubc1b\uc544\ub450\uc2dc\uace0 huggingface token\uc744 \ub4f1\ub85d\ud569\ub2c8\ub2e4. </p> <pre><code>export HF_TOKEN=HF-TOKEN\n</code></pre> <p>\uc774\uc81c vLLM \uc11c\ube59\uc744 \uc2dc\uc791\ud569\ub2c8\ub2e4. 48 \uae30\uac00\uc5d0 \uc62c\ub77c\uac00\uc9c0\ub294 \ubabb\ud560 32B \ubaa8\ub378\uc744 \uc62c\ub824\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <pre><code>vllm serve Qwen/Qwen2.5-32B-Instruct\n</code></pre> <p>\uc608\uc0c1\ud55c\ub300\ub85c OutOfMemory \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud569\ub2c8\ub2e4 !!!</p> <pre><code>torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 142.81 MiB is free....\n</code></pre> <p>2\uac1c\uc758 GPU\ub97c \ub3d9\uc2dc\uc5d0 \uc4f0\uae30 \uc704\ud574\uc11c tensor parallel \uc635\uc158\uc744 \ucd94\uac00\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <pre><code>vllm serve Qwen/Qwen2.5-32B-Instruct --tensor-parallel-size 2\n</code></pre> <p>\uc774\ubc88\uc5d4 \uc5d0\ub7ec\uac00 \uc5c6\uc774 \uc11c\ube59\uc774 \ub429\ub2c8\ub2e4.</p> <p>Runpod ID \ub85c serving \uc774 \ub418\uace0 \uc788\ub294\uc9c0 \ub0a0\ub824\ubd05\ub2c8\ub2e4, 32k \uc785\ub825\uc744 \ubc1b\ub294 Qwen2.5-32B-Instruct \uc774 \uc11c\ube59 \ub418\uace0 \uc788\uad70\uc694. </p> <pre><code>\u276f curl https://36g54goiy09px8-8000.proxy.runpod.net/v1/models\n\n{\"object\":\"list\",\"data\":[{\"id\":\"Qwen/Qwen2.5-32B-Instruct\",\"object\":\"model\",\"created\":1729343280,\"owned_by\":\"vllm\",\"root\":\"Qwen/Qwen2.5-32B-Instruct\",\"parent\":null,\"max_model_len\":32768,\"permission\":[{\"id\":\"modelperm-7eef3c565e63464d83e0ac30fb235dc0\",\"object\":\"model_permission\",\"created\":1729343280,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}%   \n</code></pre> <p>OpenAI compatible \ud55c \uc11c\ubc84\ub2c8, chat completion api \ub97c \ud638\ucd9c\ud569\ub2c8\ub2e4. \ub300\ub2f5 \ud1a0\ud070\uc774 \ub0a0\ub77c\uc635\ub2c8\ub2e4 !!</p> <pre><code>\u276f curl https://36g54goiy09px8-8000.proxy.runpod.net/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $API_KEY\" \\\n  -d '{\n    \"model\": \"Qwen/Qwen2.5-32B-Instruct\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ]\n  }'\n\n{\"id\":\"chat-20d3a3be6faf42c89dc9dbff5c1c5f01\",\"object\":\"chat.completion\",\"created\":1729343397,\"model\":\"Qwen/Qwen2.5-32B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Hello! How can I assist you today?\",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":21,\"total_tokens\":31,\"completion_tokens\":10},\"prompt_logprobs\":null}%  \n</code></pre>"},{"location":"Quantization/","title":"Quantization","text":"<p>24\ub144 10\uc6d4 \uae30\uc900, \uac00\uc7a5 \ub300\uc911\uc801\uc73c\ub85c \uc0ac\uc6a9 \ub418\ub294 Quantization \uae30\ubc95\uc740 BitsAndBytes \ub85c \ubcf4\uc785\ub2c8\ub2e4. AWQ, GPTQ \ub4f1 \ub2e4\uc591\ud55c quantization \uae30\ubc95\uacfc \ud3ec\ub9f7\ub4e4\uc774 \ub098\uc624\uace0 \uc788\uc5b4\uc694.  </p>"},{"location":"Quantization/#bitsandbytes","title":"BitsAndBytes","text":"<p>\uba3c\uc800 bitsandbytes \ub97c \uc124\uce58\ud574\uc90d\ub2c8\ub2e4. </p> <pre><code>pip install bitsandbytes&gt;=0.44.0\n</code></pre> <p>\uadf8\ub9ac\uace0, bitsandbytes \ub85c \ub9cc\ub4e4\uc5b4\uc9c4 quantized model\uc744 \ub2e4\uc6b4\ubc1b\uc544 \uc11c\ube59\ud558\uba74 \ub05d\uc785\ub2c8\ub2e4.</p> <pre><code>$ vllm serve unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit --quantization bitsandbytes --load-format bitsandbytes\n</code></pre> <p>unsloth \uc5d0\uc11c \ub9cc\ub4e4\uc5b4\uc900 4bit \ubaa8\ub378\uc744 \uc62c\ub824\ubd24\uc2b5\ub2c8\ub2e4. Unsloth \uc5d0\uc11c \uc720\uba85\ud55c \ubaa8\ub378\ub4e4\uc740 \ub2e4 \uc774\ub807\uac8c quantize \ud574\uc11c \uc62c\ub824\uc8fc\ub2c8 \uc0ac\uc6a9\ud558\uae30\uac00 \uc544\uc8fc \ud3b8\ud569\ub2c8\ub2e4. \uadf8\ub9ac\uace0, \uc9c1\uc811 quantization\uc744 \ud574\uc11c \ubaa8\ub378\uc744 \uc790\ub974\uace0 \uc2f6\uc73c\uc2e4 \ub584\uc5d0\ub3c4 unsloth\ub97c \ucd94\ucc9c\ud569\ub2c8\ub2e4. \uac1c\uc778\uc801\uc73c\ub85c \uc0ac\uc6a9\uacbd\ud5d8\uc774 \uc88b\uc2b5\ub2c8\ub2e4. \uc26c\uc6cc\uc11c\uc694.  </p> <p>\ucc38\uace0. \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9 \ube44\uad50</p> <p>\uc81c \uac1c\uc778 3090 \uba38\uc2e0\uc5d0\uc11c \ud14c\uc2a4\ud2b8\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. 8B \ubaa8\ub378\uc740 \uae30\ubcf8 \uc0c1\ud0dc\uc5d0\uc11c <code>KV cache (41152)</code> \ubc16\uc5d0 \uba54\ubaa8\ub9ac\uc5d0 \ubaa8\ub378\uc5d0 \ubabb \uc62c\ub824\uc11c context \uc904\uc774\uac70\ub098 \ud574\uc57c\ud569\ub2c8\ub2e4.</p> <pre><code>The model's max seq len (131072) is larger than the maximum number of tokens that can be stored in KV cache (41152)\n</code></pre> <p>unsloth \uc5d0\uc11c \ub9cc\ub4e4\uc5b4\uc900 4bit quantized model \uc758 \uacbd\uc6b0 \uc5ed\uc2dc \uba54\ubaa8\ub9ac\uac00 \ubaa8\uc798\ub77c\uc11c \uc62c\ub9ac\uc9c0 \ubabb\ud558\uc9c0\ub9cc, <code>KV cache (119712)</code> \uc5d0\ub7ec \uba54\uc138\uc9c0\uc5d0 \ubcf4\uc774\ub4ef \ub354 \ub9ce\uc774 \uc62c\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub354 \ud06c\uac8c context length\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc8e0.</p> <pre><code>$ vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct\n</code></pre> <pre><code>The model's max seq len (131072) is larger than the maximum number of tokens that can be stored in KV cache (119712)```\n</code></pre>"},{"location":"Quantization/#autoawq","title":"AutoAWQ","text":"<p>bnb \ub791 \ubc29\ubc95\uc740 \ub611\uac19\uc2b5\ub2c8\ub2e4. AWQ \uc124\uce58\ud574\uc8fc\uace0, AWQ\ub85c \uc798\ub824\uc9c4 \ubaa8\ub378\uc744 \uc11c\ube59\ud558\uba74 \ub429\ub2c8\ub2e4.</p> <pre><code>pip install autoawq\n</code></pre> <pre><code>$ vllm serve Qwen/Qwen2.5-32B-Instruct-AWQ --quantization awq\n</code></pre> <p>A40 (48GB) \uae30\uc900 \uc62c\ub77c\uac00\uc9c0 \uc54a\ub358 32B \ubaa8\ub378\uc774 AWQ \uc640 \ud568\uaed8\ub77c\uba74 \uad6c\ub3d9\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4 !!</p>"},{"location":"Quantization/#fp8-bf16","title":"FP8 \uc640 BF16","text":"<p>\ucd5c\uadfc \uba87\ub144\uc0c8\uc5d0 NVIDIA \uac00 \uc9c0\uc6d0\ud558\uae30 \uc2dc\uc791\ud55c quantization \uae30\ubc95\ub4e4 \uc785\ub2c8\ub2e4. \ud558\ub4dc\uc6e8\uc5b4 \uc9c0\uc6d0\uc744 \ud1b5\ud574 \uc815\uc9c1\ud55c (?) \uc131\ub2a5\ud5a5\uc0c1\uc744 \uc774\ub904\ub0c5\ub2c8\ub2e4.</p>"},{"location":"QuickStartonRunPod/","title":"QuickStartonRunPod","text":""},{"location":"QuickStartonRunPod/#_1","title":"\uc9c1\uc811 \ub3cc\ub824\ubcf4\uae30!","text":"<p>A40 (48GB) x2 \ub97c \ub300\uc5ec\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ub300\uc5ec\ub294 runpod \uc5d0\uc11c \ud588\uace0\uc694, \uc11c\ube44\uc2a4 \uc774\ub984\uc5d0\uc11c \uc54c\uc218 \uc788\ub4ef  <code>konuu/llm_ready:latest</code> docker image \ub97c \uc62c\ub824 \uc90d\ub2c8\ub2e4.  vllm image \ub97c \uc548 \uc62c\ub9b0 \uc774\uc720\ub294, model\uc744 \ucc98\uc74c\ubd80\ud130 \uac19\uc774 \ub744\uc6cc\uc8fc\ub294\uac8c \uc2eb\uc5b4\uc11c \uc785\ub2c8\ub2e4.</p> <p>(\ucc38\uace0) RTX3090 (24GB) x 4 \ub97c \uba3c\uc800 \ub300\uc5ec\ud588\ub294\ub370 \uc5d0\ub7ec\uac00 \ub098\uc11c \uc548\ub728\ub294 \uad70\uc694... ;;</p> <p>\uac1c\uc778\uc801\uc73c\ub85c\ub294 container \uac00 \uc62c\ub77c\uac04 \uac83\uc774 \uc544\ub2c8\ub77c \ud558\ub4dc\uc6e8\uc5b4\ub97c \ube4c\ub824\uc8fc\ub294 LambdaLabs \ub97c \uc120\ud638\ud558\ub294\ub370, \uc694\uc998 GPU \uac00 \ub3d9\ub098\uc11c... \ube4c\ub9ac\uae30\uac00 \ud798\ub4dc\ub124\uc694.</p> <p></p> <p>\uc9c0\uae08\uc740 vllm \uc774 \uc774\ubbf8 \uc124\uce58\ub41c \ucee8\ud14c\uc774\ub108\ub97c \ub744\uc6b0\uc9c0\ub9cc, \uc5c6\ub2e4\uba74 \uadf8\ub0e5 pip \ub85c \uc124\uce58\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4. <pre><code># Install vLLM with CUDA 12.1.\n$ pip install vllm\n</code></pre></p> <p>\uc11c\ube59\uc744 \ud55c\ubc88 \ud574\ubcf4\uc8e0, \ud658\uacbd\ubcc0\uc218 \uc14b\uc5c5\uc744 \ud55c\ubc88 \ud569\ub2c8\ub2e4.  meta-llama \ub294 \ud5c8\uae45\ud398\uc774\uc2a4\uc5d0\uc11c \uad8c\ud55c\uc744 \ubc1b\uc544\uc57c\ub9cc \uc0ac\uc6a9\uc774 \uac00\ub2a5\ud558\ub2c8, \ubbf8\ub9ac \uc2b9\uc778\uc744 \ubc1b\uc544\ub450\uc2dc\uace0 huggingface token\uc744 \ub4f1\ub85d\ud569\ub2c8\ub2e4. </p> <pre><code>export HF_TOKEN=HF-TOKEN\n</code></pre> <p>\uc774\uc81c vLLM \uc11c\ube59\uc744 \uc2dc\uc791\ud569\ub2c8\ub2e4. </p> <p>default \ub85c\ub294 8000 \ud3ec\ud2b8\ub85c api endpoint \ub4e4\uc774 \uc0dd\uae30\ub294 \ub370, runpod \uc5d0\uc11c \uc774\ub97c \ubc1b\uc744 \uc218 \uc788\uac8c, https port \ub4e4\uc744 \ubbf8\ub9ac \uc124\uc815\ud574\ub46c\uc57c \ud569\ub2c8\ub2e4!!!</p> <pre><code>vllm serve meta-llama/Llama-3.1-8B-Instruct\n</code></pre> <p>Runpod ID \ub85c serving \uc774 \ub418\uace0 \uc788\ub294\uc9c0 \ub0a0\ub824\ubd05\ub2c8\ub2e4, 131k \uc785\ub825\uc744 \ubc1b\ub294 llama3.1 \uc774 \uc11c\ube59 \ub418\uace0 \uc788\uad70\uc694. </p> <pre><code>\u276f curl https://36g54goiy09px8-8000.proxy.runpod.net/v1/models\n\n{\"object\":\"list\",\"data\":[{\"id\":\"meta-llama/Llama-3.1-8B-Instruct\",\"object\":\"model\",\"created\":1729342516,\"owned_by\":\"vllm\",\"root\":\"meta-llama/Llama-3.1-8B-Instruct\",\"parent\":null,\"max_model_len\":131072,\"permission\":[{\"id\":\"modelperm-f4239c4af18545faabe2190499d6b567\",\"object\":\"model_permission\",\"created\":1729342516,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}%   \n</code></pre> <p>OpenAI compatible \ud55c \uc11c\ubc84\ub2c8, chat completion api \ub97c \ud638\ucd9c\ud569\ub2c8\ub2e4. \ub300\ub2f5 \ud1a0\ud070\uc774 \ub0a0\ub77c\uc635\ub2c8\ub2e4 !!</p> <pre><code>\u276f curl https://36g54goiy09px8-8000.proxy.runpod.net/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $API_KEY\" \\\n  -d '{\n    \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ]\n  }'\n\n{\"id\":\"chat-076af67d9cff46659af30b617a44a490\",\"object\":\"chat.completion\",\"created\":1729342638,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Hello! How can I assist you today?\",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":43,\"total_tokens\":53,\"completion_tokens\":10},\"prompt_logprobs\":null}% \n</code></pre>"},{"location":"QuickStartonRunPod/#arguments","title":"\bArguments","text":"<p><code>vllm serve</code> \ucee4\ub9e8\ub4dc\uc758 arguments \uc911, \uc54c \ud544\uc694\uac00 \uc788\ub2e4\uace0 \uc0dd\uac01 \ub418\ub294 \ubd80\ubd84\ub4e4\ub9cc \ubf51\uc544\uc11c \uc815\ub9ac\ud569\ub2c8\ub2e4.</p> <p><code>--model</code> - \ub2f9\uc5f0\ud788 \uccab\ubc88\uc9f8\ub294 \ubaa8\ub378\uc774\uace0\uc694, hugging face \uc8fc\uc18c\ub97c \uc785\ub825\ud558\uba74 \ub429\ub2c8\ub2e4. huggingface \ub300\uc2e0 modelscope \uc73c\ub85c \uac08 \uc218\ub3c4 \uc788\uae34\ud55c\ub370, \uc911\uad6d \ucabd\uc744 \ud0c0\uac9f\uc73c\ub85c \ud558\uc9c0 \uc54a\ub294 \ud55c \uc544\ub9c8\ub3c4 huggingface \uba74 \ucda9\ubd84\ud558\uc9c0 \uc54a\uc744\uae4c \uc2f6\ub124\uc694.</p> <p><code>--disable-log-requests</code> - \ub85c\uae45 \ub044\uae30, \uc2e4\uc81c\ub85c \uc11c\ube59\ud558\uba74 \ub85c\uadf8\ub294 \uc18d\ub3c4\ub97c \uc704\ud574 \ub044\uc2dc\ub294\uac8c \uc88b\uaca0\uc8e0.</p>"},{"location":"QuickStartonRunPod/#_2","title":"\uba54\ubaa8\ub9ac \uad00\ub828","text":"<p><code>--max-model-len</code> - \ubaa8\ub378\uc758 content \uae38\uc774 \uc785\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc73c\ub85c\ub294 \ubaa8\ub378\uc5d0 \uc124\uc815\ub41c config \ub97c \ub530\ub77c\uac11\ub2c8\ub2e4. \uadf8\ub7f0\ub370, \uc694\uc998 \ubaa8\ub378\ub4e4\uc740 \uacfc\ud558\uac8c \uae34 context length \ub97c \uac00\uc9c0\uace0 \uc788\uc8e0. (Llama-3.1-8B \ub294 131k \ub97c \ubc1b\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. ) - Out-of-Memory \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud55c \uacbd\uc6b0, \uadf8\ub9ac\uace0 \uadf8\ub807\uac8c \ud070 input\uc744 \ubc1b\uc744 \ud544\uc694\ub294 \uc5c6\ub294 \uacbd\uc6b0, \uadf8\ub0e5 context length\ub97c \uc81c\ud55c \ud558\uba74 \uba54\ubaa8\ub9ac\uc5d0 \uc62c\ub824 \uc11c\ube59\uc774 \uac00\ub2a5\ud574\uc9d1\ub2c8\ub2e4.</p> <p><code>--gpu-memory-utilization</code> - GPU \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \uc9c0\uc815\ud569\ub2c8\ub2e4. vLLM \uc758 \ud575\uc2ec \uc54c\uace0\ub9ac\uc998\uc778 Paged Attention \ud2b9\uc131\uc0c1 \uba54\ubaa8\ub9ac\ub97c greedy \ud558\uac8c \uac00\uc838\uac00\uae30 \ub584\ubb38\uc5d0, \uc9c0\uc815\ud558\uba74 \uadf8\ub0e5 \ub2e4 \uba39\uc2b5\ub2c8\ub2e4, \ub2e4\ub978 GPU \uba54\ubaa8\ub9ac\uac00 \ud544\uc694\ud55c \uc77c\uc774 \uc788\ub2e4\uba74, \ub9dd\uac00\uc9c0\uaca0\uc8e0. - \uc774\ub294 0 ~ 1 \ube44\uc728\uc774\uace0, \uc804\uccb4 \uba54\ubaa8\ub9ac\uc5d0 \ub300\ud55c \ube44\uc728\uc774 \uc544\ub2c8\ub77c \ub0a8\uc544\uc788\ub294 \uba54\ubaa8\ub9ac\uc5d0 \ub300\ud55c \ube44\uc728\uc785\ub2c8\ub2e4. LLM \uc744 \uc11c\ube59\ud558\uba74 \ubcf4\ud1b5\uc740 \ud558\ub098\ub97c \ud1b5\uca30\ub85c \uac00\uc838\uac08\ud14c\ub2c8 default \uac12\uc778 0.9 \uac00 \ub098\uc058\uc9c0 \uc54a\uc740 \uac83 \uac19\uc2b5\ub2c8\ub2e4.</p> <p><code>--cpu-offload-gb</code> - CPU \uba54\ubaa8\ub9ac\uc5d0 \uc624\ud504\ub85c\ub529\ud569\ub2c8\ub2e4, 24GB GPU \uac00 \uc788\uace0, 10GB \ub97c \uc624\ud504\ub85c\ub529 \ud574\uc8fc\uba74, 34GB \uc788\ub2e4\uace0 \ubcfc \uc218 \uc788\uace0\uc694, \uadf8\ub798\uc11c 13B BF16 \ubaa8\ub378 (26GB) \ub3c4 \ub3cc\ub9b4 \uc218\uac00 \uc788\uc2b5\ub2c8\ub2e4. \ub290\ub9bd\ub2c8\ub2e4...</p>"},{"location":"QuickStartonRunPod/#multi-gpu","title":"Multi GPU \uad00\ub828","text":"<p><code>--pipeline-parallel-size, -pp</code> - \uc5ec\ub7ec \uac1c\uc758 GPU \ub85c \ubcd1\ub82c \ucc98\ub9ac \ud569\ub2c8\ub2e4. parallelism level \uc740 layer \ub2e8\uc704\ub85c\uc694. high throughput \uc5d0 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4. \uba54\ubaa8\ub9ac\uac00 \ubd80\uc871\ud55c \uacbd\uc6b0\uc5d0\ub3c4 \ud6a8\uacfc\uac00 \uc788\uc2b5\ub2c8\ub2e4.</p> <p><code>--tensor-parallel-size, -tp</code> - \uc5ec\ub7ec \uac1c\uc758 GPU \ub85c \ubcd1\ub82c \ucc98\ub9ac \ud569\ub2c8\ub2e4. parallelism level \uc740 tensor \uc5f0\uc0b0\uc5d0 \uc801\uc6a9\ub429\ub2c8\ub2e4. \uba54\ubaa8\ub9ac\uac00 \ubd80\uc871\ud55c \uacbd\uc6b0\uc5d0\ub3c4 \ud6a8\uacfc\uac00 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\ub450 \ubcd1\ub82c\uc640 \ub2e8\uacc4\uc5d0 \ub300\ud55c \ub0b4\uc6a9\uc740 Tensor parallelism vs Pipeline Parallelim  \ub97c \ucc38\uc870\ud558\uc2dc\uba74 \uc774\ud574\uc5d0 \ub3c4\uc6c0\uc774 \ub418\uc2e4 \uac83\uc785\ub2c8\ub2e4.</p>"},{"location":"QuickStartonRunPod/#quantization","title":"Quantization \uad00\ub828","text":"<p><code>--dtype</code></p> <ul> <li>\ub370\uc774\ud130 \ud0c0\uc785\uc744 \uc124\uc815\ud569\ub2c8\ub2e4. auto, half, float16, bfloat16, float, float32 \uc635\uc158 \uc911 \uace0\ub974\uba74 \ub418\ub294\ub370, \uc694\uc998 \uc624\ud508\ubaa8\ub378\ub4e4\uc740 \ub300\ubd80\ubd84 bfloat16\uc744 \uae30\ubc18\uc73c\ub85c \ud558\uace0 \uc788\uace0, \ub2e4\ub974\uac8c \ubcc0\uacbd\ub41c \ubaa8\ub378\ub4e4\uc774 \uc788\uc8e0. </li> <li>bfloat16\uc740 \uc8fc\uc758\ud560 \uc810\uc774 \uc788\ub294\ub370, \uc61b\ub0a0 nvidia gpu \ub4e4\uc774 \uc9c0\uc6d0\uc548\ud569\ub2c8\ub2e4. Tesla v100 \uac19\uc740 GPU\ub294 \uc548\ud558\ub2c8\uae4c \ud638\ud658\uc131\uc744 \uc870\uc2ec\ud558\uc138\uc694.  </li> </ul> <p><code>--quantization, -q</code> - Quantized \uc635\uc158\uc785\ub2c8\ub2e4. \ub2e4\uc74c\uacfc \uac19\uc740 \ud6c4\ubcf4\ub4e4\uc774 \uc788\uc8e0. aqlm, awq, deepspeedfp, tpu_int8, fp8, fbgemm_fp8, modelopt, marlin, gguf, gptq_marlin_24, gptq_marlin, awq_marlin, gptq, compressed-tensors, bitsandbytes, qqq, experts_int8, neuron_quant, ipex, None - \ucc38. \uc5ec\uae30\ub294 \ud1b5\uc77c\uc774 \uc548\ub418\uc5b4\uc11c \uac11\uac11\ud55c\ub370\uc694, \uadf8\ub798\ub3c4 bnb \uac00 \uac00\uc7a5 \ub9ce\uc774 \uc0ac\uc6a9\ub418\ub294 \uac83 \uac19\uace0\uc694, \ub9cc\ub4e4\uc5b4\uc9c4 \ubaa8\ub378\uc744 \uc798 \ubcf4\uace0 \ub9de\ucdb0\uc8fc\uc2dc\uba74 \ub429\ub2c8\ub2e4.  - Bitsandbytes \uac00 \ud5c8\uae45\ud398\uc774\uc2a4\uc5d0\uc11c \uac00\uc7a5 \ub300\uc911\uc801\uc778 \ud3ec\ub9f7\uc774\ub098, \ud6a8\uc728\uc131\uc774\ub098 \uc131\ub2a5\uc774 \uc81c\uc77c \uc88b\uc740 \uac83\uc740 \uc544\ub2c8\ub77c\uace0 \ubcf4\uc5ec\uc9d1\ub2c8\ub2e4. - AWQ \ub098 GPTQ \ub97c \ub9ce\uc774 \uc0ac\uc6a9\ud558\ub294 \ucd94\uc138 \uc785\ub2c8\ub2e4. - \uc9c1\uc811 quantization \ud558\uc2e4 \ubd84\ub4e4\uc740 \uc774\ubbf8 \uc798 \uc544\uc2e4\ud14c\ub2c8 \uc790\uc138\ud55c \uc124\uba85\uc740 \uc0dd\ub7b5\ud558\uace0\uc694, \ub300\ubd80\ubd84\uc740 inference \uc11c\ube59\uc744 \uc2dc\uc791\ud558\uae30 \uc804\uc5d0 \ub2e4\uc591\ud558\uac8c quantiazation \ud55c \ud6c4, \ubaa8\ub450 \ud14c\uc2a4\ud2b8\ub97c \ub3cc\ub824\ubcf4\uace0, \uc131\ub2a5/\ud6a8\uc728\uc744 \ube44\uad50\ud558\uc5ec \uacb0\uc815\ud558\ub294 \uac83 \uac19\uc2b5\ub2c8\ub2e4.</p> <p><code>--load-format</code> - \ubaa8\ub378\uc758 \ud30c\ub77c\ubbf8\ud130\ub4e4 \ud3ec\ub9f7\uc744 \uc9c0\uc815\ud574\uc90d\ub2c8\ub2e4, \uaf2d Quantization \uc774\ub791\ub9cc \uad00\ub828\uc774 \uc788\ub294 \uac83\uc740 \uc544\ub2c8\uc9c0\ub9cc, quantized \ubaa8\ub378\uc744 \ub85c\ub4dc\ud558\ub294 \uacbd\uc6b0\uc5d0 \uc704 -q \uc635\uc158\uc774\ub791 \uc798 \ub9de\ucdb0\uc8fc\uc9c0 \uc54a\uc73c\uba74 \uc5d0\ub7ec\uac00 \uc798 \ub098\uae30 \ub584\ubb38\uc5d0 \uc774 \ud30c\ud2b8\uc5d0 \uc124\uba85\ud569\ub2c8\ub2e4. - bnb \ubaa8\ub378\uc744 \ub85c\ub4dc\ud558\uc2dc\uba74 \uc5ec\uae30\ub3c4 \ub611\uac19\uc774 bnb \ub9de\ucdb0\uc8fc\uc2dc\uba74 \ub429\ub2c8\ub2e4. - \uac00\ub2a5 \uc635\uc158\ub4e4:  auto, pt, safetensors, npcache, dummy, tensorizer, sharded_state, gguf, bitsandbytes, mistral</p>"},{"location":"QuickStartonRunPod/#lora","title":"LoRA \uad00\ub828","text":"<p><code>--enable-lora</code> - LoRA \uae30\ub2a5\uc744 \ucf1c\uc90d\ub2c8\ub2e4.</p> <p><code>--max-loras</code> - LoRA \ucd5c\ub300 \uac2f\uc218 \uc9c0\uc815\ud569\ub2c8\ub2e4, (\uae30\ubcf8\uc740 1)</p> <p>```</p>"},{"location":"RunpodServerless/","title":"RunpodServerless","text":"<p>Runpod \uc740 GPU pod\uc744 \ub300\uc5ec\ud574\uc8fc\uae30\ub3c4 \ud558\uc9c0\ub9cc, Serveless \uc635\uc158\ub3c4 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc544\uc8fc \uc27d\uac8c ondemand LLM \uc11c\ube44\uc2a4\ub97c \ub3cc\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\ucc38\uc870 - https://github.com/runpod-workers/worker-vllm</p> <p></p> <p>vLLM \uc774 \uc544\ub2cc Stable Diffusion \uc774\ub098 \uc784\ubca0\ub529, (\ucea1\ucc98\uc5d0\ub294 \ube60\uc84c\uc9c0\ub9cc) whisper \ub3c4 \ud638\uc2a4\ud305\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <p>\uc544\ub798\uc640 \uac19\uc740 \uc124\uc815\ucc3d\ub4e4\uc744 \uc9c0\ub098\uace0 \ub098\uba74 \uc778\uc2a4\ud134\uc2a4\uac00 \uc0dd\uae30\uac8c \ub429\ub2c8\ub2e4.</p> <ol> <li>Model \uacfc vLLM \uc124\uc815. \uad8c\ud55c\uc774 \ud544\uc694\ud558\ub2e4\uba74, HF token \uae4c\uc9c0 \uc124\uc815\ud574\uc8fc\uba74 \ub429\ub2c8\ub2e4.</li> </ol> <p></p> <ol> <li> <p>\ub450\ubc88\uc9f8\ub294 vLLM \uc124\uc815\uc785\ub2c8\ub2e4. \uc544\uc8fc \uc544\uc8fc \ub9ce\uc740 \uc14b\ud305 \uc694\uc18c\ub4e4\uc774 \uc788\ub294\ub370, vLLM engine\uc758 argument \ub4e4\uc785\ub2c8\ub2e4.     \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 QuickStart \ud398\uc774\uc9c0\ub97c \ubcf4\uc2dc\uba74 CLI \uc0c1\uc5d0\uc11c\uc758 \uc124\uba85\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uc640 \ub611\uac19\uc740 \uc635\uc158\ub4e4\uc774 \uac70\uc758 \uadf8\ub300\ub85c UI \ub85c \uad6c\uc131 \ub41c \uac83\uc774\ub2c8 \ud544\uc694\uc5d0 \ub9de\uac8c \uac12\ub4e4\uc744 \uc785\ub825\ud574\uc8fc\uba74 \ub429\ub2c8\ub2e4. </p> </li> <li> <p>vLLM \uc774 \uc544\ub2cc serverlss setting \ub780\uc774 \ub354 \uc788\ub294\ub370, \ub85c\uae45\uc774\ub791 \uc5bc\ub9c8\ub098 \ub3d9\uc2dc \uc785\ub825\uc744 \ucc98\ub9ac\ud560 \uac83\uc778\uac00\uc5d0 \ub300\ud55c \uc124\uc815\uc785\ub2c8\ub2e4.</p> </li> </ol> <p></p> <ol> <li>\ub9c8\uc9c0\ub9c9\uc740 GPU \uc124\uc815\uc744 \ud558\ub294\ub370, \uba54\ubaa8\ub9ac\ub97c \uae30\uc900\uc73c\ub85c \uc9c0\uc815\ub418\uc5b4\uc788\uc2b5\ub2c8\ub2e4. \uba54\ubaa8\ub9ac\uac00 \uace7 \uc62c\ub9b4 \uc218 \uc788\ub294 \ubaa8\ub378\uc744 \uacb0\uc815\ud558\ub2c8 \uc774\ub807\uac8c \ub9cc\ub4e4\uc5c8\ub098 \ubd05\ub2c8\ub2e4. \uac19\uc740 16\uae30\uac00 GPU \uc5ec\ub3c4 \uc5ec\ub7ec\uac00\uc9c0 GPU \uac00 \uc788\uc744 \uc218 \uc788\ub294\ub370, \uc54c\uc544\uc11c \ub0a8\ub294 \uc790\uc6d0 \ubcf4\uace0 \ub744\uc6cc\uc90d\ub2c8\ub2e4.</li> </ol> <p></p> <p>\uc774\ub807\uac8c \uc124\uc815\uc744 \ud558\uace0 \ub098\uba74 endpoint \uac00 \uc0dd\uc131\ub429\ub2c8\ub2e4.  \uc694\uccad\uc744 \ud55c\ubc88 \ub0a0\ub824\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. </p> <pre><code>curl -X POST https://api.runpod.ai/v2/{endpoint_id}/runsync \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer ${API_KEY}' \\\n    -d '{\"input\": {\"prompt\": \"\uc624\ub298 \uc800\ub141 \uba54\ub274 \ucd94\ucc9c \uc880..!\"}}'\n</code></pre> <pre><code>{\"delayTime\":6074,\"executionTime\":257,\"id\":\"sync-8716c00d-783a-4698-a6a1-9f7eeb9a1150-e1\",\"output\":[{\"choices\":[{\"tokens\":[\" \uc624LCETOHex\ub97c \u0642\u0631\ufffd\ub2c8.._production \uc911\uc5d0\\toLCET\"]}],\"usage\":{\"input\":11,\"output\":16}}],\"status\":\"COMPLETED\",\"workerId\":\"4lff10gwwrz14w\"} \n</code></pre> <p>\ub77c\uace0 \ub2f5\ubcc0\uc774 \uc624\ub294\uad70\uc694. serving \ud558\ub294 \ubaa8\ub378\uc774 <code>meta-llama/Llama-3.2-1B-Instruct</code> \ub77c\uc11c \uc5b4\ub9ac\ubc84\ub9ac \ub300\ub2f5\uc744 \ud588\uad70\uc694.</p> <p>\uc774\ubc88\uc5d4 serverless \ubaa8\ub378\ub3c4 3.1-8B \ubaa8\ub378\ub85c \ubc14\uafd4\uc8fc\uace0, openai compatible \ud55c chat/completions API endpoint \ub85c \ub0a0\ub824\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <pre><code>curl \"https://api.runpod.ai/v2/vllm-q7f8fz5ac1mro0/openai/v1/chat/completions\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer {API_KEY}\" \\\n    -d '{\n        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"\uc624\ub298 \uc800\ub141 \uba54\ub274 \ucd94\ucc9c \uc880..!\"\n            }\n        ]\n    }'\n</code></pre> <pre><code>{\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"\uc624\ub298 \uc800\ub141 \uba54\ub274\ub97c \ucd94\ucc9c\ud574 \ub4dc\ub9b4\uac8c\uc694!\\n\\n1.  **\uac08\ube44\ud0d5**: \ud55c\ubcf5\ud310\uc5d0 \ub300\ud45c\uc801\uc778 \uc74c\uc2dd \uc911 \ud558\ub098\uc778 \uac08\ube44\ud0d5\uc740 \uae30\ub984 \uc9c4 \uac08\ube44\uc640 \ud568\uaed8 \ub053\uc5ec\ub0b8 \uad6d\ubb3c\ub85c \uac00\ub4dd\ud55c \uc74c\uc2dd\uc785\ub2c8\ub2e4. \uac08\ube44\uc640 \uad6d\ubb3c\uc774 \ud568\uaed8\ud558\ub294 \ub9db\uc788\ub294 \uc74c\uc2dd\uc774\uc694!\\n2.  **\ubd88\uace0\uae30**: \uc18c\uace0\uae30\u0e2b\u0e23\ufffd\uac70\ub098 \uc1e0\uace0\uae30\ub97c \ud1b5\uc9f8\ub85c \ubd88\uace0\uae30\ubc18\uc5d0 \uac04\uc7a5, \uc124\ud0d5, b\u00e0n mu\u1ed1i \ub4f1\uc73c\ub85c \uc870\ub9ac\ud558\uc5ec\u719f\ub828\ub41c \ub9db\uc744 \ub0b4\ub294 \uc74c\uc2dd\uc785\ub2c8\ub2e4.\\n3.  **\ubd88\ub2ed\uac08\ube44**: \uac08\ube44\ub97c \ubd88\ub2ed\uc7a5\uacfc \uc11e\uc5b4\uac04 \ud6c4\uc5d0 \ubd88\ub824\ub0b8 \uad6d\ubb3c\uc5d0\u0e43\u0e2a\uc6cc\uc11c \uad6d\ubb3c\uc774 \uac00\ub4dd\ud55c \uc74c\uc2dd\uc785\ub2c8\ub2e4.\\n4.  **\uce58\ud0a8**: \uce58\ud0a8\uc740 \ubbf8\uad6d\uc5d0\uc11c \uc720\ub798\ub41c \uc74c\uc2dd\uc73c\ub85c \uae30\ub984\uc5d0 \ud280\uaca8\ub0b8 \ub2ed \uace0\uae30\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4. \uac04\ud639 \uce58\ud0a8\uc758 \uc694\ub9ac\ubc95\uc740 \ub9db\uc744 \ub0b4\ub294 \ubd80\ubd84\uc744 \ub9d0\uc500\ub4dc\ub9ac\uc790\uba74, \uae30\ub984\uc5d0 \ud280\uaca8\ub0b8 \ud6c4\uc5d0 \ub9c8\ub298\uacfc \ud6c4\ucd94, \uc0dd\uac15 \ub4f1\uacfc \ud568\uaed8 \uac04\uc7a5\uc744 \ubfcc\ub824 \uadf8 \ub9db\uc744 \ub0b4\ub294 \uc74c\uc2dd\uc785\ub2c8\ub2e4.\\n5.  **\ud574\ubb3c\ud30c\uc2a4\ud0c0**: \ud574\ubb3c\ud30c\uc2a4\ud0c0\ub294 \ud30c\uc2a4\ud0c0\uc5d0 \ud574\ubb3c\ucc0c\uac1c\ub97c \uc5b9\uc5b4 \uc9d1\uc5d0\ub9cc \uc788\ub294 \ud574\ubb3c\ucc0c\uac1c\uc758 \ub9db\uc744 \ub0b4\ub294 \uc74c\uc2dd\uc785\ub2c8\ub2e4. \ud574\ubb3c\ucc0c\uac1c\uc758 \ub9db\uc744 \ub0b4\ub294 \ud574\ubb3c\ucc0c\uac1c\ub294 \ud574\ubb3c\ucc0c\uac1c\uc5d0 \ud574\ubb3c\ucc0c\uac1c\ub97c \uaf2d \ub123\uc5b4\uc11c \ub053\uc5ec\ub0b8 \ud6c4\uc5d0 \ud30c\uc2a4\ud0c0\uc5d0 \uc5b9\uc5b4 \uc8fc\ub294 \uc74c\uc2dd\uc785\ub2c8\ub2e4.\\n\\n\uc704\uc758 \uba54\ub274 \uc911 \ud558\ub098\ub85c \uc120\ud0dd\ud574 \uc8fc\uc2dc\uba74 \uc88b\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4.\",\"role\":\"assistant\",\"tool_calls\":[]},\"stop_reason\":null}],\"created\":1729425952,\"id\":\"chat-3fecdfddffe041fd8a0c33b701a3d3f9\",\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"object\":\"chat.completion\",\"prompt_logprobs\":null,\"usage\":{\"completion_tokens\":357,\"prompt_tokens\":51,\"total_tokens\":408}}\n</code></pre> <p>\uc774\ub807\uac8c \ub2f5\ubcc0\uc774 \uc654\uad70\uc694 ...!</p> <p>\uc544\ub798\uc640 \uac19\uc774 - API endpoint \ub4e4, \ud604\uc7ac inference \uc591\uc5d0 \ub530\ub77c \ucc28\uc9d5\ub418\ub294 \uae08\uc561 - \uc2e4\uc2dc\uac04\uc73c\ub85c worker\ub4e4\uc774 \uc62c\ub77c\uac14\ub2e4 \ub0b4\ub824\uac14\ub2e4, \ub85c\uadf8\uc640 \ud568\uaed8 metrics \ub4e4\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p></p> <p>UI \uc0c1 \uc81c\uacf5\ud558\ub294 \ub0b4\uc6a9\ub4e4\uc774 \uc0ac\uc2e4 \ud6e8\uc52c \ub9ce\uc740\ub370, \uc601\uc0c1 \ud615\ud0dc\uc5ec\uc57c \uc798 \ubcf4\uc77c \uc218 \uc788\uaca0\ub354\uad70\uc694.</p> <p>\uc694\uc57d\ud558\uc790\uba74, - \ub204\uad6c\ub098 Open\ub41c \ubaa8\ub378\uc744 \uc6d0\ud074\ub9ad\uc73c\ub85c \ub3cc\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. - \uc2ec\uc9c0\uc5b4 On-Demand \ub85c \ub3cc\uace0\uc694, - \uc5b4\ub290 \uc815\ub3c4 Auto-Scaling \ub3c4 \ub429\ub2c8\ub2e4. - OpenAI Compatible \ud574\uc11c \ud638\ud658\uc131\ub3c4 \uc88b\uc2b5\ub2c8\ub2e4.</p> <p>\ucca8\uc5b8 \uc870\uae08\ub9cc \ud558\uc790\uba74 \ubbf8\ucce4\ub124\uc694 \uadf8\ub0e5. LambdaLabs \uc5d0 GPU \uac00 \ub3d9\ub098\uc11c runpod\uc73c\ub85c \uc774\uc0ac\uc654\ub294\ub370, \ub108\ubb34 \ub9ce\uc740 \uac83\ub4e4\uc774 \uc27d\uac8c \ub429\ub2c8\ub2e4.</p>"},{"location":"Scaling/","title":"Scaling","text":""},{"location":"Scaling/#multi-node","title":"Multi - Node","text":"<p>\uc774\ubc88\uc5d4 \ub354 \ud070 \ubaa8\ub378\uc744 \ub3cc\ub9ac\uace0 \uc2f6\uc5b4\uc694. 70B \ubaa8\ub378\uc744 \uc62c\ub824\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <pre><code>vllm serve meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 2\n</code></pre> <p>\uc5ed\uc2dc OutOfMemory \uac00 \ubc1c\uc0dd\ud569\ub2c8\ub2e4.</p> <pre><code>torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 64.81 MiB is free....\n</code></pre> <p>\uba40\ud2f0 \ub178\ub4dc\ub85c \ud655\uc7a5\uc744 \ud574\uc57c\ud569\ub2c8\ub2e4. (\uc0ac\uc2e4 70B \ubaa8\ub378\uc740 GPU\ub97c \uadf8\ub0e5 \ub9ce\uc774 \ub2ec\uc544\ub3c4 \ub418\uae34 \ud558\uc9c0\ub9cc \uba40\ud2f0 \ub178\ub4dc \ud14c\uc2a4\ud2b8\ub97c \uc704\ud55c \uc2e4\uc2b5\uc785\ub2c8\ub2e4) 2\uac1c\uc758 \ucee8\ud14c\uc774\ub108\ub97c \ub744\uc6cc \uc90d\ub2c8\ub2e4. </p> <p></p> <p>2\uac1c\uc758 \ub3c4\ucee4 \uc774\ubbf8\uc9c0\ub97c \uc5f0\uacb0\ud560 \uac83\uc778\ub370, host \uc640 worker \ub97c \ubaa8\uc544\uc11c \ud074\ub7ec\uc2a4\ud130\ub97c \uad6c\uc131\ud574\uc57c\ud569\ub2c8\ub2e4. vllm cluster\ub294 run_cluster.sh \uc2a4\ud06c\ub9bd\ud2b8\ub97c \ucc38\uc870\ud574\uc11c ray \ub85c \uad6c\uc131\ud588\uc2b5\ub2c8\ub2e4. </p> <p>LambdaLabs \ub3c4 \uadf8\ub807\uace0, RunPod \ub3c4 \uadf8\ub807\uace0, \ud074\ub7ec\uc2a4\ud130 host\ub97c \uc548\ucabd\uc5d0 \ub450\uba74 \uc6cc\ucee4\ub178\ub4dc\ub4e4\uc774 \uc811\uc18d\ud558\uae30\uac00 \uc5b4\ub824\uc6cc\uc694. Firewall \uc124\uc815\uc774 \uc870\uae08 \ubcf5\uc7a1\ud569\ub2c8\ub2e4. \uadf8\ub798\uc11c host\ub294 \ud655\uc2e4\ud558\uac8c public ip \ub85c \uc81c\uac00 \ub2e4 \ub124\ud2b8\uc6cc\ud06c \ucee8\ud2b8\ub864 \uad8c\ud55c\uc744 \uac00\uc9c4 \ud648\uc11c\ubc84\ub85c \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4. </p> <p>\ubc84\uc804 \ud638\ud658\uc744 \uc704\ud574 \ud648\uc11c\ubc84\uc5d0 \ub611\uac19\uc774 \ub3c4\ucee4 \uc774\ubbf8\uc9c0\ub97c \uc62c\ub824\uc90d\ub2c8\ub2e4. (\ubc84\uc804\uc774 \uc870\uae08\ub9cc \ub2ec\ub77c\ub3c4 \ud074\ub7ec\uc2a4\ud130 \uad6c\uc131\uc774 \uc798 \uc548\ub429\ub2c8\ub2e4.)</p> <pre><code>docker run -it -p 6379:6379 konuu/llm_ready:latest /bin/bash\n</code></pre> <p>\uadf8\ub9ac\uace0, \ud638\uc2a4\ud2b8\uac00 \ub420 \ub178\ub4dc\ub97c \ucf1c\uc90d\ub2c8\ub2e4. </p> <pre><code>ray start --block --head --port=6379\n</code></pre> <p>RunPod \uc5d0\uc11c \ub744\uc6b4 \ub450 \uac1c\uc758 GPU pod \ub4e4\uc5d0\uc11c host \ub85c \uc5f0\uacb0 \ud574\uc90d\ub2c8\ub2e4.  <pre><code>ray start --block --address='jonhpark.iptime.org:6379'\n</code></pre></p> <p>\uadf8\ub9ac\uace0, <code>ray status</code> \ub97c \ucc0d\uc5b4\ubcf4\uba74 \uc798 \uc5f0\uacb0\uc774 \ub418\uc5c8\ub294\uc9c0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <pre><code>root@b9aa63cc7a54:/# ray status\n======== Autoscaler status: 2024-10-19 16:55:27.171800 ========\nNode status\n---------------------------------------------------------------\nActive:\n 1 node_f7f6fb1cc9bc0458714ac8368cee8da2191feebd5701ca46ba920903\n 1 node_7af94f3b6d67a3c0477ec12a39970f8dd3cb0b9527db400a9ce044a0\n 1 node_b818e9149e8de461e242348ad7bc98a0f8ea91cf1854132061590284\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/36.0 CPU\n 0.0/4.0 GPU\n 0B/134.45GiB memory\n 0B/57.94GiB object_store_memory\n</code></pre> <p>\uadf8\ub7f0\ub370, \uc790\uafb8 \uae08\ubc29 \uc8fd\uc5b4\uc11c \ubd88\uc548\ud558\ub124\uc694. \uc544\ubb34\ub798\ub3c4 1CC \uac00 \ud544\uc694\ud55c\ub370, 1\uc8fc\uc77c\uc529 reserve \ub97c \ud574\uc57c\ud558\uae30 \ub584\ubb38\uc5d0, lambda labs \uae30\uc900 12000 \ub2ec\ub7ec\uac00 \ud544\uc694\ud569\ub2c8\ub2e4... \u3160\u3160 </p>"},{"location":"Scaling/#skypilot","title":"SkyPilot","text":"<pre><code># for MACOS \n# pip\u00a0uninstall\u00a0grpcio;\u00a0conda\u00a0install\u00a0-c\u00a0conda-forge\u00a0grpcio=1.43.0\n\n\nconda create -y -n sky python=3.10\nconda activate sky\n\npip install \"skypilot[runpod]\"\n</code></pre> <p>runpod \uae30\uc900</p> <p>api key \ubc1c\uae09 \ud6c4</p> <p>pip install \"runpod&gt;=1.5.1\" runpod config</p>"},{"location":"VisionModel/","title":"VisionModel","text":"<p>Image \ub97c input\uc73c\ub85c \uc81c\uacf5\ubc1b\ub294 \ubaa8\ub378\uc744 \uc11c\ube59\ud558\uace0 \ud14c\uc2a4\ud2b8 \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. OpenAI Vision API \uc640 \ud638\ud658\uc774 \uac00\ub2a5\ud558\uac8c \uc11c\ube59\uc744 \ud558\uba74, GPT-4V \ub098 GPT-4o\ub85c \uad6c\ud604\ub41c LLM application \uc5d0 \ud638\ud658\uc774 \uc88b\uaca0\uc8e0.</p> <pre><code>vllm serve microsoft/Phi-3.5-vision-instruct \\\n  --trust-remote-code --max-model-len 4096 --limit-mm-per-prompt image=2\n</code></pre> <p>OpenAI Vision API \ub294 Chat Completion \uc2a4\ud0c0\uc77c\uc5d0 \ucd94\uac00\uac00 \ub418\uc5b4\uc788\uc8e0, \uadf8\ub798\uc11c vllm \uc73c\ub85c \uc11c\ube59\uc2dc\uc5d0 Chat Template \uc774 \ud544\uc218 \uc785\ub2c8\ub2e4. \uc704\uc5d0\uc11c \uc0ac\uc6a9\ud55c Phi-3.5-vision-instruct \ubaa8\ub378\uc740 chat template \uc774 \ub0b4\uc7a5\uc774\uae30 \ub54c\ubb38\uc5d0 \ubb38\uc81c\ub294 \uc5c6\uc9c0\ub9cc, \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc740 \ud655\uc778\uc744 \ud574\uc57c\ud569\ub2c8\ub2e4.  </p> <p>\uc544\ub798\uc640 \uac19\uc740 \uc774\ubbf8\uc9c0\ub97c \uc785\ub825\ud558\ub294 \uc608\uc2dc\ub85c \uc0ac\uc6a9\ud558\uba74 chat completion openai API \ub85c \ubc14\ub85c \uc0ac\uc6a9\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.  </p> <pre><code>from openai import OpenAI\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\n# Single-image input inference\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n\nchat_response = client.chat.completions.create(\n    model=\"microsoft/Phi-3.5-vision-instruct\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            # NOTE: The prompt formatting with the image token `&lt;image&gt;` is not needed\n            # since the prompt will be processed automatically by the API server.\n            {\"type\": \"text\", \"text\": \"What\u2019s in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n        ],\n    }],\n)\nprint(\"Chat completion output:\", chat_response.choices[0].message.content)\n\n# Multi-image input inference\nimage_url_duck = \"https://upload.wikimedia.org/wikipedia/commons/d/da/2015_Kaczka_krzy%C5%BCowka_w_wodzie_%28samiec%29.jpg\"\nimage_url_lion = \"https://upload.wikimedia.org/wikipedia/commons/7/77/002_The_lion_king_Snyggve_in_the_Serengeti_National_Park_Photo_by_Giles_Laurent.jpg\"\n\nchat_response = client.chat.completions.create(\n    model=\"microsoft/Phi-3.5-vision-instruct\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What are the animals in these images?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url_duck}},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url_lion}},\n        ],\n    }],\n)\nprint(\"Chat completion output:\", chat_response.choices[0].message.content)\n</code></pre> <p>\ucc38\uace0!</p> <p>https://github.com/vllm-project/vllm/pull/7916</p>"}]}